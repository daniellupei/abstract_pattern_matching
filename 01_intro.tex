\section{Introduction}

%(Motivation)

Event-series pattern matching has become essential to many data processing tasks
as it enables complex behavioral, anomaly, and causality analyses, in varied
domains ranging from network diagnostics and security breach detection, to
algorithmic trading or click-path optimization.  This trend prompted the
addition of pattern matching constructs to many batch and stream processing
engines such as Esper's Event Processing Language (EPL)~\cite{esper_epl},
Oracle's \texttt{MATCH\_RECOGNIZE}~\cite{oracle_mr}, TerraData's \texttt{nPath}
operator~\cite{aster_npath} or Splunk's Search Processing
Language~\cite{Carasso:2012}.

These languages let programmers express patterns as a series of transitions,
wherein a transition 
is triggered if the current event satisfies a guard expressed in terms of both 
unary(selection) predicates as well as join conditions on priorly matched 
events. 
For example, a programmer might mine
influential reviews within the click-stream of an e-commerce website consisting
of events of type ``Search''(S), ``Read review''(R) and ``Purchase''(P) by
expressing the pattern SR*P where each transition is joined on a userid (i.e.,
to make sure that all events in a match are correlated by the id of the user 
that performed them).

%(Challenges)
Pattern matching is usually only one of many stages in a data processing
pipeline.  As most of these stages are defined using relational queries (i.e.,
to enrich ingested data), the presence of pattern matching operators raises
considerable challenges in terms of deriving optimum execution plans.  Pattern
matching operators require their input be should be sorted on time and in
addition do not enjoy the same wealth of rewriting rules and optimization
opportunities as traditional relational operators.  As a consequence, just
getting data sorted often takes a significant amount of data center resources
even if matching the patterns themselves is fast.

Modern warehoused data is typically processed by a mix of workloads.  For
example, such systems have to service non-temporal queries (i.e., in which city
are located most of the last month's website visitors).  These non-temporal
queries have very different optimum data layout, and as the latter kind are
usually much more frequent, their optimum data layout ends up becoming the
layout of choice in the data center.  Keeping a second copy of the data sorted
on time is wasteful when dealing with terabytes of data, especially in the light
of the fact that many of the recorded events may not even be of interest to the
mined pattern.  Further, because data are collected from a wide array of
sources, each with varying constraints on when data ingestion occurs, sorting
the world for every pattern matching query is a frequent and costly occurrence
in the datacenter.
% That is why, before sorting the data and feeding
% it to a pattern matching engine, an extra preprocessing step is commonly used to
% discard all the events that do not satisfy any of the selection predicates of
% the pattern.  We also note that the input data is not necessarily sorted on time
% in the first place either, as it may be collected from a wide array of sources,
% with varying constraints for when the data ingestion should happen (for eg. when
% mining the activity of a user on multiple devices).  Finally, we remark that the
% requirement that the input be ordered by time is especially taxing when
% executing on a map-reduce platform, as the sorting step incurs an expensive
% reshuffling of the entire data.
%Analytics are being run both over fresh data as well as historical data,
%therefore the data management system needs to support both online and
%batching mode processing.
%

%(Approach)
In this work we demonstrate how to optimize a class of temporal queries on
non-sorted data, thus reducing the cost of \texttt{ORDER BY time}. In
particular, this paper introduces {\em abstract pattern matching}, a technique
that builds cheap and effective filters that remove a significant amount of data
\emph{before} sorting the data by time.  Furthermore, our filters are themselves
represented in relational algebra so the optimizer can include them in its
optimization of the entire pipeline.

% In this work we address both challenges by exploiting the fact that a large 
% class of patterns can be equivalently expressed as relational queries.
% More precisely, we target patterns specified via the usual operators of regular 
% expressions: concatenation, union and Kleene star, on top of event variables 
% annotated by guards, i.e.\ logical formulas deciding which complex events in 
% the input can bind to that particular event variable.
% The class of patterns that we translate to relational queries restricts the use 
% of the Kleene star only over sub-patterns of fixed length.
% Nonetheless, this class captures the overwhelming majority of patterns 
% encountered across benchmarks, both industrial or proposed in the 
% literature.    

% Translating patterns to relational queries opens the door for relational 
% optimizers to operate also over the pattern matching stages of a data 
% processing pipeline while searching for the cheapest execution plan.
% Moreover, it adds an array of optimization opportunities (for eg., performing 
% partial aggregations) that would otherwise have been missed due to the fact 
% that for the most part the  pattern matching operator is treated as a black-box.


% However, not all patterns can be translated into relational expressions and 
% even for those that can, the translation may produce queries that are deemed 
% more expensive to evaluate than the standard pattern matching operator. 
% The latter is more likely to happen for complex patterns considering that the 
% translation introduces a join and a nested query for each variable in the 
% pattern. 
% For such cases we propose the technique of {\em abstract pattern matching} as a 
% way of discarding from the input those events that cannot participate in any 
% successful match, even before they are considered by the pattern matcher. 
% This technique is prompted by the fact that usually only a tiny fraction of the 
% input events match a given pattern and as such is meant to alleviate the 
% overhead incurred due to the sorting/shuffling of the entire data (i.e.\ the 
% \texttt{ORDER BY time}  clause) prior to being processed by a pattern matching 
% engine.

% While it is standard practice to preprocess the input by filtering out those 
% events that do not match any of the selection predicates of a pattern (i.e.\ 
% the unary predicates or those predicates testing an event variable against 
% constants), the idea behind {\em abstract pattern matching} is to also leverage 
% the filtering power of the join predicates, as well as that of the dependencies 
% captured by the pattern itself. 
To gain an intuition for our approach, consider the earlier pattern SR*P for
mining influential reviews.  Abstract pattern matching first builds three
independent sets of user ids: a set of user ids for users that (S)earched for a
product, a set of user ids for users that (R)ead at least one review, and a set
of user ids for users that (P)urchased the product.  The intersection of these
sets is a sound and conservative over-approximation of the set of users that
will ultimately take part in the final match and thus those user ids not in this
intersection can be filtered from the input (i.e., it is an over-approximation
because it ignores time).  % Such a filter
% is an example of a bloom-join~\cite{Bloom:1970} and when represented as such can
% provide a concise and fast way to filter rows from the input.  
This work formalizes and generalizes this intuition to more complex patterns
that deal with multiple join (theta) predicates.

Abstract pattern matching first associates to every transition in a pattern a
{\em symbolic set} capturing the domain of its join attributes based on those
input events that satisfy its selection predicates.  It then refines these
symbolic sets by enforcing the join predicates
between different transitions as well as the structure of the pattern.
Finally, it selects from the input only those events that satisfy the resulting
set of constraints, which we refer to as the {\em abstract filter}.

Since the precise representation of the symbolic sets could in many cases be
just as large as the input, we introduce {\em data and predicate abstractions}
to compute and query them in a time and space efficient manner.  Depending on
the type of join constraints that we have to propagate for a particular
transition, {\em data abstraction} makes use of appropriate abstract set
representations that can conservatively approximate those constraints (for
example, for equijoins we make use of Bloom filters\cite{Bloom:1970}, whereas
for inequality/band joins we rely on interval maps).  (i.e., as in
Figure~\ref{fig:dabstraction}).  In addition, predicate abstraction further
reduces the overheads of our approach by dropping in a sound way some of the
transitions specified by the pattern.  For example, few users ultimately purchase
a product and so we can soundly over-approximate the set of users that may take
part in an influential review by \emph{only} computing that set (i.e., as in
Figure~\ref{fig:pabstraction}).

In concert, data and predicate abstraction allow us to cope with join predicates
that do not have efficient data abstractions, as well as fine-tune our solution
such that it only considers the most selective join predicates and balance the
overhead of building our filters with their selectivity.
\begin{figure}
  \centering
  \begin{subfigure}{\columnwidth}
    \centering    
    \includegraphics[clip, page=1,width=\columnwidth]{graphs/motivation.pdf}
    \vspace{-3cm}
    \caption{Intervals are a compact over-approximation of users that search.}
    \label{fig:dabstraction}
  \end{subfigure}
  %
  \begin{subfigure}{\columnwidth}
    \includegraphics[clip, page=2,width=\columnwidth]{graphs/motivation.pdf}
    \caption{SR*P will only match those users that search, possibly read
      some reviews, and finally, purchase a product.  Predicate abstraction
      exploits the selectivity of one predicate (most users do not purchase) to
      efficiently over-approximate users that match SR*P.}
    \label{fig:pabstraction}
  \end{subfigure}
  \caption{Examples illustrating the core intuitions underlying data and 
  predicate abstraction.}
\end{figure}
      
 
Our approach is inspired by the concept of abstract 
interpretation~\cite{Cousot:1977,Graf:1997}.  
In particular, the {\em abstract filter} constraints are the result of
relaxing/coarsening in a conservative manner of the precise constraints enforced
by the pattern regarding which events form successful matches.  This suggests
future work can leverage the technique of abstract interpretation to optimize
other user defined aggregates as well, (i.e.\ to derive abstract filters meant 
to remove from the input those tuples that are guaranteed not to contribute to 
an output of interest).

% abstract interpretation provides a template for how  this optimization can be 
%extended to other udf running in the reducer stages of a map-reduce pipeline 

%(Results)
As previously mentioned, constructing and applying the abstract filter incurs a
series of overheads, most notably it requires a second pass over the data.
Nonetheless, if the reduction in data is significant, these additional costs are
balanced out by the dramatic speedup in the sorting/shuffling/pattern matching
phases which results in an overall decrease in both processing costs and
latency.  Our experimental evaluation shows up to 3 orders of magnitude
reduction in shuffled data as well as 1.23x average speedup in total processing
time for 2 workloads: i) telemetry analysis over the events produced by an
event-reporting infrastructure, and ii) repository analysis over the dataset of
events published by GitHub.  The reduction in total processing time is
especially important in multi-tenant clusters where clients are billed based on
the amount of computational resources they consume.

The contributions of this paper are:
\begin{itemize}
	\item We show how a significant class of complex event patterns can be 
	translated to relational queries such that they can benefit from decades of 
	progress in relational optimizations.
	\item We introduce the technique of {\em abstract pattern matching} in 
	order to minimize the sorting/shuffling costs of large scale mining of 
	patterns within map-reduce frameworks.
	\item We design {\em data and predicate abstractions} that allow us to 
	trade the precision of our approach (but not its soundness) for lower 
	overheads.
	%\item We highlight how the concepts of symbolic execution and abstract 
	%interpretation can be used to design similar optimizations for other 
	%user-defined reduce operators.
	\item We prototype our solution and show on an industrial benchmark that 
	it delivers significant reductions in the amount of data sorted/shuffled as 
	well as processing times.  
\end{itemize}


The rest of the paper is organized as follows: 
we further illustrate our approach and detail its design in 
sections~\ref{sec:mot_example} and~\ref{sec:design},
the related work is discussed in section~\ref{sec:rel_work},  
the choices we made in implementing our solution are explored in 
section~\ref{sec:implementation}, followed by the presentation of the results 
of our experimental evaluation in section~\ref{sec:evaluation}. 
Finally, section~\ref{sec:conclusions} gives our concluding remarks and 
comments on future directions.   











\section{Motivating example}
\label{sec:mot_example}

Consider an analytics task that mines influential reviews
within the click-stream of an e-commerce website consisting of events of type
``Search''(S), ``Read review''(R) and ``Purchase''(P).
The mining task is described in terms of the pattern SR*P, with each event 
variable annotated by its own guard:
\begin{align*}
\text{{\bf S}} \in \Ev&: \facc{S}{name} = ``S"
\\
\text{{\bf R}} \in \Ev&: (\facc{R}{user} = \facc{S}{user} \rightarrow
\facc{R}{name} = ``R")
\\
\text{{\bf P}} \in \Ev&: (\facc{P}{name} = ``P") . 
(\facc{P}{user} = \facc{S}{user}) . 
(\facc{P}{t} < \facc{S}{t} + t_{out}),
\end{align*}
where we consider as input an $\Ev$(name, t, user) relation with fields for 
event name, event timestamp and associated user id, and we use $.$ as a 
shorthand for the conjunctive operator. 
The pattern is described in terms of both selection predicates (for identifying 
the name of the event to be matched), as well as join predicates that make sure 
that the matched events are correlated by the id of the user that performed 
them and that the ``Purchase'' event occurs within a timeout $t_{out}$ from the 
``Search'' event. 
\begin{comment}
In addition, figure~\ref{fig:srp_pattern} presents the symbolic automata 
representation of the pattern, i.e.\ the state machine where each transition 
has an associated event variable and a corresponding guard.
\end{comment}

The typical execution plan of this task on a map-reduce framework is to first
group all events by user id, then sort them on time and finally run a pattern
matching engine to detect the desired sequence of events.
In this work we propose to greatly expand the array of possible execution plans
by taking advantage of the fact that a large class of patterns can be
equivalently expressed as SQL queries.
By doing so then one can leverage decades of progress in query optimization to
come up with more efficient query plans than the one outlined above.
For instance, we can express our example pattern as a SQL query as follows:
{\small
	\begin{verbatim}
	SELECT S.*, P.*
	FROM Ev S, Ev P
	WHERE S.time < P.time
	AND S.name == "S"   
	AND P.name == "P" AND P.time < S.time + t_out
	AND P.user == S.user
	AND NOT EXISTS ( 
	SELECT * FROM Ev R
	WHERE S.time < R.time AND R.time < P.time
	AND R.user == S.user
	AND R.name != "R" ); 
	\end{verbatim}
}

The first part of the query enforces the fact that a successful match consists
of an event S followed within a timeout $t_{out}$ by an event P from the same 
user,
while the second part captures the fact that only R events are allowed to take
place in between these two events.
The final result of the query is a set of tuples, one per successful match,
consisting of the initial and final event in the match.

% Since the number of purchase events P are likely to be much lower than the 
%search
% or read review events, performing a broadcast join wrt.\ to the P events might
% prove to be the optimum execution plan for this pattern. One of the key 
%insights
% of this paper is that representing a pattern as a SQL query provides 
%significant
% query optimization opportunities.

% The class of patterns that we can currently translate into SQL are those whose
% automata have only cycles of constant length, ie. every repetition
% within a particular cycle should consist of the same number of events. 
% Note this class includes the overwhelming majority of patterns discussed
% in the literature of complex event processing systems as well as encountered
% within an industrial benchmark.

% While we acknowledge that for patterns with many transitions, whose
% corresponding queries involve many joins, the optimizer might find optimal the
% standard query plan based on the pattern matching engine,
% we argue that even in such cases one can still leverage the derived query to
% minimize the number of events that get grouped by and sorted before being fed 
%to
% the pattern matching engine.
% Ideally, only the events that will end up as part of a successful match should
% undergo this process.
In the following we discuss our approach for performing this pre-processing step
in a time and space efficient manner.  

We start by defining for each event variable $X$ of the pattern a 
{\em symbolic set} $\syms{X}$, which collects the values of $X$'s fields that 
are joined by the query, based on the input events matching its selection 
predicates. 
\begin{align*}
\syms{S} 
&\coloneq 
\{ \tuple{\facc{S}{t}, \facc{S}{user}} \mid 
S \in \Ev: \facc{S}{name} = \text{``S''}
\}
\\
\syms{\oR} 
&\coloneq 
\{ \tuple{\facc{R}{t}, \facc{R}{user}} \mid 
R \in \Ev: \facc{R}{name} \neq \text{``R''}
\}
\\
\syms{P} 
&\coloneq 
\{ \tuple{\facc{P}{t}, \facc{P}{user}} \mid 
P \in \Ev: \facc{P}{name} = \text{``P''}
\}
\end{align*}
We also re-write the query, first using comprehension syntax, 
and then using a slicing operator:
\begin{align*}
&
Q \coloneq 
\{ \tuple{\symv{s}, \symv{p}} \mid 
\symv{s} \in \syms{S},
\symv{p} \in \syms{P}: 
\\ 
&
\qqquad\quad\;\;
\facc{\symv{p}}{t} \in
(\facc{\symv{s}}{t} : \facc{\symv{s}}{t} + t_{out})\ .\ 
(\facc{\symv{p}}{user} = \facc{\symv{s}}{user})\ .\ 
\\
&\qqquad\quad\;\;
\neg
(\exists
\symv{r} \in \syms{\oR} : 
\facc{\symv{r}}{t} \in (\facc{\symv{s}}{t} : \facc{\symv{p}}{t})\ .\ 
(\facc{\symv{r}}{user} = \facc{\symv{s}}{user})
)
\ \}
\\
&
\;\;\; \coloneq 
\{ \tuple{\symv{s}, \symv{p}} \mid 
\symv{s} \in \syms{S},
\symv{p} \in 
\slice{\syms{P}}
{(\facc{\symv{s}}{t}\,:\,\facc{\symv{s}}{t} + t_{out}),
	\facc{\symv{s}}{user}}
: 
\\ 
&\qqquad\quad\;\;
\slice{\syms{\oR}}
{(\facc{\symv{s}}{t}\,:\,\facc{\symv{p}}{t}),\; \facc{\symv{s}}{user}} 
= \emptyset  
\ \}
\end{align*}
where 
$\slice{\syms{P}}
{(\facc{\symv{s}}{t}\,:\,\facc{\symv{s}}{t} + t_{out}),
	\facc{\symv{s}}{user}}
$
denotes the slicing of $\syms{P}$ defined as:
\begin{align*}
\{ \symv{p} \mid 
\symv{p} \in \syms{P} : 
\facc{\symv{p}}{t} \in (\facc{\symv{s}}{t} : \facc{\symv{s}}{t} + t_{out})\ .\ 
(\facc{\symv{p}}{user} = \facc{\symv{s}}{user})
\},
\end{align*}
and similarly for
$\slice{\syms{\oR}}
{(\facc{\symv{s}}{t}\,:\,\facc{\symv{p}}{t}),\; \facc{\symv{s}}{user}}$,
with the latter allowing us to express the \texttt{NOT EXISTS} sub-clause as an 
emptiness test.


\begin{comment}
\begin{align*}
\{ \symv{r} \mid 
\symv{r} \in \syms{\oR} : 
\facc{\symv{r}}{t} \in (\facc{\symv{s}}{t} : \facc{\symv{p}}{t})\ .\ 
(\facc{\symv{r}}{user} = \facc{\symv{s}}{user})
\},
\end{align*}



\begin{figure}[t]
\centering
\includegraphics[clip, trim=0cm 10cm 0cm 3cm,width=\columnwidth]
{graphs/example_sm2.pdf}
\caption{Symbolic automata for the SR*P pattern.}
\label{fig:srp_pattern}
\end{figure}

\end{comment}

Each symbolic set, in concert with the slice operator let us remove events from
the input that are guaranteed not to participate in a successful match.
% and as such minimize the number of events being sorted and considered by the 
% pattern matcher, 
For this example pattern we get the following filters:
\begin{align*}
\precs{S}(\symv{s}) 
&\equiv  
\exists \symv{p} \in 
\slice{\syms{P}}
{(\facc{\symv{s}}{t}\,:\,\facc{\symv{s}}{t} + t_{out}),
	\facc{\symv{s}}{user}} :\ 
\\
&\qquad\quad
\slice{\syms{\oR}}
{(\facc{\symv{s}}{t}\,:\,\facc{\symv{p}}{t}),\; \facc{\symv{s}}{user}}
= \emptyset 
\\
\precs{\oR}(\symv{r}) 
&\equiv
\slice{Q}
{(-\infty\,:\,\facc{\symv{r}}{t}), 
	\facc{\symv{r}}{user}, 
	(\facc{\symv{r}}{t}\,:\,\infty), 
	\facc{\symv{r}}{user}} 
\neq \emptyset 
\\
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
&\equiv
\exists \symv{s} \in 
\slice{\syms{S}}{(-\infty\,:\,\facc{\symv{r}}{t}), \facc{\symv{r}}{user}},
\\
&\quad
\exists \symv{p} \in 
\slice{\syms{P}}
{(\max(\facc{\symv{s}}{t},\facc{\symv{r}}{t})\,:\,\facc{\symv{s}}{t} + t_{out}),
	\facc{\symv{r}}{user}}
: 
\\ 
&\qquad\quad
\slice{\syms{\oR}}
{(\facc{\symv{s}}{t}\,:\,\facc{\symv{p}}{t}),\; \facc{\symv{r}}{user}} 
= \emptyset  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\\
\precs{P}(\symv{p}) 
&\equiv  
\exists \symv{s} \in 
\slice{\syms{S}}
{(\facc{\symv{p}}{t} - t_{out}\,:\,\facc{\symv{p}}{t}),  
	\facc{\symv{p}}{user}} :\
\\
&\qquad\quad
\slice{\syms{\oR}}
{(\facc{\symv{s}}{t}\,:\,\facc{\symv{p}}{t}),\; \facc{\symv{p}}{user}}
= \emptyset,
\end{align*}
which when applied to their corresponding symbolic set will retain only those 
events that contribute to the output of $Q$. 

Because evaluating these filters would in many cases be as expensive as 
computing
the complete result $Q$, we use them only as a starting point for deriving a set
of {\em abstract filters}, as a ``relaxed'' version of these
{\em precise filters}, but that can be applied with low processing and 
communication costs.
We do so by employing a series of {\em data and predicate abstractions} 
that generate conservative versions of the original filters.

\newcommand{\NotExistsP}{\ident{NotExistsP}}
\newcommand{\PrecedesP}{\precs{S}}
\newcommand{\PrecedesPP}{\omega_S}
\newcommand{\PrecedesPPP}{\psi_S}
\newcommand{\PrecedesPPPP}{\upsilon_S}


\newcommand{\interval}[1]{\lfloor #1 \rceil}
\newcommand{\uinterval}[1]{\lceil #1 \rfloor}
\newcommand{\hashid}[1]{\# #1}

% We showcase our techniques on the filter corresponding to the S event 
%variable,
% which highlights the fact that the only Search events in the output are 
%those  
% that have only Read-review events between themselves and the next Purchase 
% events occurring within a $t_{out}$ window of time.

\begin{comment}
we first complement in order to obtain the set of "Search" events that we 
want discarded.
\begin{align*}
\PrecedesP(\symv{s}) 
&\equiv  
\forall \tuple{\facc{\symv{p}}{t}, \_} \in 
\slice{\syms{P}}
{(\facc{\symv{s}}{t}\,:\,\facc{\symv{s}}{t} + t_{out}),
\facc{\symv{s}}{user}} :\ 
\\
&\qquad
\slice{\syms{\oR}}
{(\facc{\symv{s}}{t}\,:\,\facc{\symv{p}}{t}),\; \facc{\symv{s}}{user}}
\neq \emptyset 
\end{align*}
\end{comment}

User's that search for a product must (i) optionally read a review and
ultimately, (ii) purchase a product \emph{within} a window of $t_{out}$
time. \emph{Data abstraction} computes in a time and space efficient
representation of sets $\syms{S}$ and $\syms{\oR}$.  For example, one could
abstract over time and coarsen timestamps $t$ into time intervals
$\interval{t}$.  Then, the time dimension of sets $\syms{S}$, $\syms{\oR}$ could
be efficiently encoded and queried as interval maps (i.e., bit vectors where
each bit corresponds to a time interval and a set bit would denote the fact that
an event has occurred within the corresponding interval).  Using this
abstraction the filter becomes:
\begin{align*}
\PrecedesP^{\interval{t}}(\symv{s}) 
&\equiv  
\exists \symv{p} \in 
\slice{\syms{P}}
{\interval{\facc{\symv{s}}{t}\,:\,\facc{\symv{s}}{t} + t_{out}},
	\facc{\symv{s}}{user}} :\ 
\\
&\qquad\quad
\slice{\syms{\oR}}
{\uinterval{\facc{\symv{s}}{t}\,:\,\facc{\symv{p}}{t}},\; 
	\facc{\symv{s}}{user}}
= \emptyset 
\end{align*}
where in order to maintain conservativeness (i.e.,
$\PrecedesP \rightarrow \PrecedesP^{\interval{t}} $) we must over-approximate
the $\facc{\symv{s}}{t}:\facc{\symv{s}}{t} + t_{out}$ range but
under-approximate the $\facc{\symv{s}}{t}:\facc{\symv{p}}{t}$ interval.  Thus,
in order to be sound (and, depending on the filter) data abstractions may
provide both over- and under- approximations of the original sets.


There are many different ways to approach data abstraction.  For example, 
hashing
is one approach to abstract over sets.  If we use a compact representation of
sets $\syms{S}$, $\syms{\oR}$ that stores time information only per user hash
bucket as opposed to individual user ids, then the resulting abstract filter:
\begin{align*}
&
\PrecedesP^{\hashid{user}}(\symv{s}) \equiv 
\exists \symv{p} \in 
\slice{\syms{P}}
{(\facc{\symv{s}}{t}\,:\,\facc{\symv{s}}{t} + t_{out}),
	\hashid{\facc{\symv{s}}{user}}} :\ 
\\
&\qqquad\qquad\quad\;\,
\slice{\syms{\oR}}
{(\facc{\symv{s}}{t}\,:\,\facc{\symv{p}}{t}),\; 
	\hashid{\facc{\symv{s}}{user}}}
= \emptyset 
\end{align*}
does not satisfy our safety requirement 
(i.e.\ $\PrecedesP \nrightarrow \PrecedesP^{\hashid{user}}$).
This happens because hashing can only provide over approximations of sets while,
in order to ensure conservativeness, the abstractions used for this filter need
to provide both over and under approximations.
While there are several ways to address this issue (for an alternative solution 
see section~\ref{sec:data_abstraction}), 
in the following we show how {\em predicate abstraction} can alleviate the 
problem.


Predicate abstraction encompasses a set of re-writings 
that relaxes the filter by discarding those constraints that cannot be safely
or efficiently abstracted over.
For instance, our filter could be weakened into:
\begin{align*}
&
\PrecedesPPP(\symv{s}) \equiv 
\slice{\syms{P}}
{(\facc{\symv{s}}{t}\,:\,\facc{\symv{s}}{t} + t_{out}),\; 
	\facc{\symv{s}}{user}}
\neq \emptyset,
\end{align*} 
which eliminates only those search events that are not followed by a purchase
event within $t_{out}$, and was obtained from the base case of
the existential quantifier in $\PrecedesP$.
Since this version only requires over-approximation, one can safely use hashing
to abstract over the user id dimension of $\syms{P}$, i.e.\ 
$\PrecedesPPP \rightarrow \PrecedesPPP^{\hashid{user}}$, where
\begin{align*}
&
\PrecedesPPP^{\hashid{user}}(\symv{s}) \equiv 
\slice{\syms{P}}
{(\facc{\symv{s}}{t}\,:\,\facc{\symv{s}}{t} + t_{out}),\; 
	\hashid{\facc{\symv{s}}{user}}}
\neq \emptyset .
\end{align*}

Moreover, predicate abstraction also reveals the well known Bloom
join algorithm as an instance of our approach, wrt.\ the join
predicate $\facc{P}{user} = \facc{S}{user}$ from the original query. 
This becomes apparent if we ignore time in the filter above:
\begin{align*}
&
\PrecedesPPP^{\hashid{user}}(\symv{s}) \equiv 
\slice{\syms{P}}{*, \hashid{\facc{\symv{s}}{user}}}
\neq \emptyset,
\end{align*}
and we use a Bloom filter to implement 
$\slice{\syms{P}}{*, \hashid{\facc{\symv{s}}{user}}}$.
More importantly, it highlights the fact that one can use data and predicate 
abstraction to explore the entire spectrum of abstract filters, and make the 
choice between precision vs overheads on a case by case basis.
We discuss additional scenarios where predicate abstraction proves
beneficial in section~\ref{sec:pred_abstraction}.




\begin{comment}
\\
&
\NotExistsP(t_S, user_S) \coloneq 
E_P(t_S..t_S+t_{out}, user_S) = \emptyset

These filters are designed to be applied independently over the input without
the need to perform any joins, group by's or order by's.
More importantly, in a map-reduce framework they can be performed with minimal
communication overhead.


- interval map may use different granularities in different regions of the
timeline, i.e. finer granularity in regions with higher probability of events
\end{comment}




































